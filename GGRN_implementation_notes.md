#### Pitfalls

1. A linear function $F$ is not uniquely determined by equations of the form $y=F^2(x)$, because it can be replaced by $-F$. Because of this, it will be important to always have at least a few training examples where $y=F(x)$ (usually at least the controls, and sometimes all samples if steady state is assumed). This becomes less of an issue if we have irregularly spaced time-points, or if we shrink $F$ towards the identity function (absent evidence, prefer steadiness over oscillation).
2. It is probably hard to train this type of model when $S>2$. A possible work-around/relaxation would be to add some intermediates: instead of minimizing $||Y - F^4(X)||^2$, minimize $||Y - H^2(X)||^2 + \lambda||F^2(X) - H(X)||^2$. For linear models, $H=F^2$ is possible exactly; for neural nets, $H$ would act as an approximation for $F^2$, but probably shallower. 
3. A different work-around (linear only) would be to represent $F$ via its eigendecomposition. 
4. Yet another relaxation would be to minimize $||Y - F_1(F_2(X))||^2 + \lambda||F_1(X) - F_2(X)||^2$.
5. Another possible pitfall is if $z\neq Q((z))$. Adding $||Q(R(z)) - z||^2$ to $J$ could help, or $R$ could be absorbed into $G$ as in DCD-FG.
6. If you use L-BFGS rather than ADAM, PyTorch allows you to set the learning rate and do gradient clipping. This makes no sense with L-BFGS. Use LR=1 with no gradient clipping.

#### Software testing

The vanilla version of this is easy to test: generate random controls X and a random transition matrix G; do some perturbations; apply G to them; and feed it into the software. Do not specify the controls as having reached steady state (or else modify the generating procedure so that they do reach it). Manually specify the matching of the treated samples to the right controls. It's also easy to do similar tests while restricting nonzeroes in G to known interactions.

The low-rank structure options can be tricky to test. We describe different training options case by case.

- Fixed: This seems simple as long as we use the same $Q$ and $R$ to during training that we used to generate the data.
- Supervised: The complication here is that $R_1G_1Q_1=R_2G_2Q_2$ does not imply $R_1=R_2$, etc. We can test that $F$ is correctly recovered, but we can't expect to get individual factors right.
- PCA: The complication here is that generative models corresponding to PCA don't allow interventions. But, we can design a special case with all the PC's nicely axis-aligned, and with no centering or scaling (SVD rather than PCA). Let $Q = R^T = [\sqrt{2},\sqrt{2}] \otimes I $ where $\otimes$ is the Kronecker product. Let $G$ be diagonal with entries $[2,3,4,5,6]$. Initialize $X_0$ to an identity matrix. The final data are $RG^SQ$, since $QR=I$ and $X_0=I$. This is already in the form of an SVD (with singular values correctly ordered), so Q and R will be learned correctly up to sign flips. 

#### Optimization notes

As of 2022 Dec 27, the code is in an interesting state where it converges to the right answer on low-D toy examples some of the time, and other times it displays a couple different problems. Sometimes it refuses to converge and other times it is making progress but rapidly blows up to very large weights or biases. Here is what we have begun to experiment with.

- Size and dimension of simulated data
- Values of S up to 7 (mostly 1 and 2)
- Random seed (after which everything is deterministic)
- Initialization of G 
- L1 regularization strength
- Early stopping
- Gradient clipping
- Learning rate
- L-BFGS versus Adam ([see also](https://soham.dev/posts/linear-regression-pytorch/))

Anecdotally:

- higher S and higher dimension makes training harder. 
- Regular initializations designed for independent layers (e.g. kaiming or xavier) do not work as well for our model, which is structured more like a recurrent neural network. A better default is the identity matrix, used by Geoff Hinton's group in [this RNN initialization paper](https://arxiv.org/pdf/1504.00941.pdf).
- Early stopping is often too early. It's often described as a way to prevent overfitting: you stop when the validation loss increases, even if the training loss could decrease further ([example](https://medium.com/pytorch/pytorch-lightning-1-3-lightning-cli-pytorch-profiler-improved-early-stopping-6e0ffd8deb29)). Right now, I just want to fit the training data really well, but I'm using early stopping because there seems to be no other way to test for convergence when using pytorch lightning. UPDATE: there are tolerance params in torch.optim.LBFGS and they are probably just so small that I haven't run into them yet.

Formal experiments:

- Experiments 1-4 accidentally used the same seed on all repeats so they have effectively no replication. But they seem to show that "kaiming" is worse than "identity" init (best honest option), and "identity" is worse than "user" (which is done by cheating) (see logs 2 & 4). Also, that higher dimensions (range: 2 to 10 and S: 2) make this a harder problem (log 3). 
- Experiment 5 has 5 replicates and confirms that "kaiming" is worse than "identity" init, and "identity" is worse than "user".
- Experiment 6 has 5 replicates and shows that shuffling the data is harmful. 
- Experiment 7 has 5 replicates and shows that early stopping sometimes helps and sometimes hurts. Longer runtime will probably eventually be better if we can solve the problem where certain runs diverge. 
- Experiment 8 shows that setting the learning rate to 1 instead of 0.0005 makes it worse, even though L-BFGS doesn't have a learning rate parameter. This makes no sense and it was with early stopping so I continued with lr=1 regardless. lr=1 is the PyTorch default for L-BFGS. L-BFGS has a similar parameter called a step length, which is either fixed at 1 (pytorch default) or found at each iteration via binary search. 
- Experiment 9 is a redo of experiment 8 with no early stopping. Small learning rate works well 3/5 times and diverges 2/5. Learning rate of 1 never converges and never diverges. 
- Experiment 10 is a redo of experiment 9 with line search on vs off instead of setting LR to 1 versus small. LR is fixed at 1. There is no early stopping. Without line search, 0/5 converged. Line search kicks ass, converging nicely on 4/5 random seeds. The worst performer was seed=2. The transition matrix was [[0.89, 0.12],    [0.76, 0.17]], which is ill-conditioned (eigenvalues of 1 and 0.06) and far from the identity (which is the current initialization). The loss became stuck at 6.9 and remained stuck there throughout training. This happened even when the regularization parameter was set to 0. The value of G that it gets stuck at is [[0.9705113 , 0.10846261], [0.41104037, 1.0194758 ]] . ADAM does not get stuck on this problem, though it takes ~7,000 epochs to get the loss below 0.1.
- Experiment 11 tests ADAM versus L-BFGS on the same 5 random datasets (including seed 2, the nasty case just discussed). ADAM is worse and does get stuck on the problem case (seed=2), which contradicts the above -- possibly due to early stopping. 
- Experiment 12 tries to rectify the problem via better settings of Lightning's early stopping mechanism. With better early stopping params, ADAM converges 5/5 times and L-BFGS only 4. Maybe we are ready to make it harder. We will use ADAM until this question is revisited, since it works 5/5 times in this experiment.
- Experiment 13 increases the dimension. 2d is easy, 10d is ok, and weirdly, 5d is hard.
- Experiment 14 further increases the dimension to 20, 50, 100 (but makes it easier by decreasing S to 1). Total error is hard to interpret because in higher dimensions, the same per-entry error will result in a larger total error. So, I switched to mean instead of sum over absolute error: `np.abs(Ghat - G).mean()`. (This was previously labeled L2 error but that was a mistake; the code previously read `np.abs(Ghat - G).sum()`, which is L1, not L2.) To see what happens during training, I also added the experiment name and a timestamp to the logger (currently using the pytorch default, which is tensorboard). The results are actually pretty good even up at dimension 100. I partly convinced myself of this by manually checking that the mean absolute value of the error $G-\hat G$ is only about 10% of the mean absolute value of either the true $G$ or the estimate $\hat G$.
- Experiment 15 is like experiment 14, but I set the batch size to be larger than the data (it was supposed to be single-batch all along) and I cranked the dimensions up further (200, 500, 1000). It seems to have worked ok for 200D but bad for 500 and 1000. For 500, the logs seem to show very early termination, but interacting now, I cannot reproduce this. This is still S=1, so we're just doing linear regression with N/P = 10 and no noise. Why would it not work? 
- Experiment 16 took a detour and tested out ADAM versus ADAMW, amsgrad, and L-BFGS. It was a simple little experiment with S=1 and dimension=2. All five got good convergence, but L-BFGS appeared to DOMINATE on runtime. I only logged number of epochs, which could be unfair to ADAM, but in an undocumented followup, I also logged walltime and L-BFGS was 500-fold better. It also never got stuck as seen in experiment 10, maybe because right now S=1 and back then S was 2. Switching S to 2 does seem to reproduce the L-BFGS stuckness on seed=2. 
- Experiment 17 tests L-BFGS vs ADAM with S=1 on a variety of problem dimensions. I realized that the high-D experiments might be getting harder and harder because for each experiment, I scale G to have a max eigenvalue of 1. So the entries get really small by the time we get to dimension 500 or 1000. In terms of walltime, ADAM starts out WAY worse (more than 100-fold), but L-BFGS's advantage decreases with the dimension, and they are nearly on par for 500D. ADAM also seems to scale sub-linearly with the problem dimension, and I would expect that to break down and to not be relevant to the real problem scale. 
- Exp 18: The default memory for L-BFGS in PyTorch is 100. This will naturally slow it down more and more as either the dimension or the number of steps grows beyond 100. It is possible that a good Hessian approximation is less important than other aspects of L-BFGS, such as the Wolfe line search. Thus, experiment 18 tests L-BFGS with some smaller memories. We use dimension 100 and we use S=1 for this experiment. The memory makes little difference to the quality or runtime; perhaps longer memory helps a little with both but the trend is very noisy.
- Exp 19: For high dimensions, during experiment 17, I noticed the problem caused by normalizing the largest eigenvalue to 1. In exp 19 and beyond, I remove that normalization step and G becomes just IID uniform entries with whatever spectrum that implies. This regime yielded very accurate estimates in terms of relative error, even up to 1000D.
- Exp 20: can we now extend this work to yield accurate estimates for S>=2? Experiment 20 tests lots of values of S on a small 10D example. They work very well for S=1 and S=4, but fail badly on S=2,8,16. I accidentally overwrote these results but see exp 21.
- Exp 21: To address pitfall number 1 ($G^2 = (-G)^2$), I repeat experiment 20 but generate $G$ to be closer to the identity matrix ($G = I + U$ where $U$ is iid uniform on $[0,1]$). The L1 penalty shrinks estimates towards $I$. The model is unlikely to recover $-G$ since $-G$ is farther from the identity. At S=1,2,4, this worked nicely. At S=8,16, it mostly did not converge, and at S=16, it mostly stopped immediately. 
- Exp 22: This is exp 21, but with ADAM in place of L-BFGS. It works up to S=8. At S=16, it stops at epoch 31, likely because the early stopping patience param is 30. 
- Exp 23: This is exp 22 but with S=4 fixed and with various dimensions. It worked pretty well up to dimension=50. At dimension=100, it seems promising but it hit the max num epochs (10k) after ~1.5 hours. At dimension 200 and 500, ADAM stopped at bad estimates after thousands of epochs. I was able to interact with the results from the final run (500D). The entries of G^S are large (typical absolute value 8E6), and the relative error in estimating G^S is under 1%. Relative error in estimating $G$ is close to 50%. I suspect Pitfall #1 again, since a matrix with 500 distinct eigenvalues has many 4th roots (500^4 of them). I also notice that the true G has 203 eigenvalues with negative real part, and the estimated G has none. I consider this a success, and I will begin logging errors in $G^S$ alongside errors in $G$, for cases where $G$ is not identifiable.

After experiment 23, I committed changes and began working on additional features. 