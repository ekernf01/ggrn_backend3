These notes describe implementation progress on the autoregressive backend. For a formal mathematical specification of the objective functions we want to minimize, visit the ggrn repo. 

#### Pitfalls

1. A linear function $F$ is not uniquely determined by equations of the form $y=F^2(x)$, because it can be replaced by $-F$. Because of this, it will be important to always have at least a few training examples where $y=F(x)$ (usually at least the controls, and sometimes all samples if steady state is assumed). This becomes less of an issue if we have irregularly spaced time-points, or if we shrink $F$ towards the identity function (absent evidence, prefer steadiness over oscillation).
2. It is probably hard to train this type of model when $S>2$. A possible work-around/relaxation would be to add some intermediates: instead of minimizing $||Y - F^4(X)||^2$, minimize $||Y - H^2(X)||^2 + \lambda||F^2(X) - H(X)||^2$. For linear models, $H=F^2$ is possible exactly; for neural nets, $H$ would act as an approximation for $F^2$, but probably shallower. 
3. A different work-around (linear only) would be to represent $F$ via its eigendecomposition. 
4. Yet another relaxation would be to minimize $||Y - F_1(F_2(X))||^2 + \lambda||F_1(X) - F_2(X)||^2$.
5. Another possible pitfall is if $z\neq Q((z))$. Adding $||Q(R(z)) - z||^2$ to $J$ could help, or $R$ could be absorbed into $G$ as in DCD-FG.
6. If you use L-BFGS rather than ADAM, PyTorch allows you to set the learning rate and do gradient clipping. This makes no sense with L-BFGS. Use LR=1 with no gradient clipping.

#### Correctness testing

Before deploying on real data we test the software for correctness on data generated exactly according to model assumptions. The vanilla version of this is easy to test: generate random controls X and a random transition matrix G; do some perturbations; apply G to them; and feed it into the software. Do not specify the controls as having reached steady state (or else modify the generating procedure so that they do reach it). Manually specify the matching of the treated samples to the right controls. It's also easy to do similar tests while restricting nonzeroes in G to known interactions.

The low-rank structure options can be tricky to test. We describe different training options case by case.

- Fixed: This seems simple as long as we use the same $Q$ and $R$ to during training that we used to generate the data.
- Supervised: The complication here is that $R_1G_1Q_1=R_2G_2Q_2$ does not imply $R_1=R_2$, etc. We can test that $F$ is correctly recovered, but we can't expect to get individual factors right.
- PCA: As above, $R_1G_1Q_1=R_2G_2Q_2$ does not imply $R_1=R_2$, etc. We can test that $F$ is correctly recovered, but we can't expect to get individual factors right except by coincidence. The complication here is that even though people use PCA for GRN inference, the mathematical models corresponding to PCA don't normally discuss interventions or time-series. For us, a time-series $X_0, P(F(P(X_0))), (PF)^2(P(X_0))...$ will in general be confined to the column space of $F$, except for $X_0$ and except for any sample that is intervened on ($P(X)\neq X$). Normally our testing code will output $X_0$ and $X_1$. To get $F$ exactly right, we can set $X_0$ to something in $col(F)$, or to a multiple of the identity matrix (because that will preserve the left singular vectors). 

#### Optimization notes: vanilla linear autoregressive

As of 2022 Dec 27, the code is in an interesting state where it converges to the right answer on low-D toy examples some of the time, and other times it displays a couple different problems. Sometimes it refuses to converge and other times it is making progress but rapidly blows up to very large weights or biases. Here is what we have begun to experiment with.

- Size and dimension of simulated data
- Values of S up to 7 (mostly 1 and 2)
- Random seed (after which everything is deterministic)
- Initialization of G 
- L1 regularization strength
- Early stopping
- Gradient clipping
- Learning rate
- L-BFGS versus Adam ([see also](https://soham.dev/posts/linear-regression-pytorch/))

Anecdotally:

- higher S and higher dimension makes training harder. 
- Regular initializations designed for independent layers (e.g. kaiming or xavier) do not work as well for our model, which is structured more like a recurrent neural network. A better default is the identity matrix, used by Geoff Hinton's group in [this RNN initialization paper](https://arxiv.org/pdf/1504.00941.pdf).
- Early stopping is often too early. It's often described as a way to prevent overfitting: you stop when the validation loss increases, even if the training loss could decrease further ([example](https://medium.com/pytorch/pytorch-lightning-1-3-lightning-cli-pytorch-profiler-improved-early-stopping-6e0ffd8deb29)). Right now, I just want to fit the training data really well, but I'm using early stopping because there seems to be no other way to test for convergence when using pytorch lightning. UPDATE: there are tolerance params in torch.optim.LBFGS and they are probably just so small that I haven't run into them yet.

Formal experiments:

- Experiments 1-4 accidentally used the same seed on all repeats so they have effectively no replication. But they seem to show that "kaiming" is worse than "identity" init (best honest option), and "identity" is worse than "user" (which is done by cheating) (see logs 2 & 4). Also, that higher dimensions (range: 2 to 10 and S: 2) make this a harder problem (log 3). 
- Experiment 5 has 5 replicates and confirms that "kaiming" is worse than "identity" init, and "identity" is worse than "user".
- Experiment 6 has 5 replicates and shows that shuffling the data is harmful. 
- Experiment 7 has 5 replicates and shows that early stopping sometimes helps and sometimes hurts. Longer runtime will probably eventually be better if we can solve the problem where certain runs diverge. 
- Experiment 8 shows that setting the learning rate to 1 instead of 0.0005 makes it worse, even though L-BFGS doesn't have a learning rate parameter. This makes no sense and it was with early stopping so I continued with lr=1 regardless. lr=1 is the PyTorch default for L-BFGS. L-BFGS has a similar parameter called a step length, which is either fixed at 1 (pytorch default) or found at each iteration via binary search. 
- Experiment 9 is a redo of experiment 8 with no early stopping. Small learning rate works well 3/5 times and diverges 2/5. Learning rate of 1 never converges and never diverges. 
- Experiment 10 is a redo of experiment 9 with line search on vs off instead of setting LR to 1 versus small. LR is fixed at 1. There is no early stopping. Without line search, 0/5 converged. Line search kicks ass, converging nicely on 4/5 random seeds. The worst performer was seed=2. The transition matrix was [[0.89, 0.12],    [0.76, 0.17]], which is ill-conditioned (eigenvalues of 1 and 0.06) and far from the identity (which is the current initialization). The loss became stuck at 6.9 and remained stuck there throughout training. This happened even when the regularization parameter was set to 0. The value of G that it gets stuck at is [[0.9705113 , 0.10846261], [0.41104037, 1.0194758 ]] . ADAM does not get stuck on this problem, though it takes ~7,000 epochs to get the loss below 0.1.
- Experiment 11 tests ADAM versus L-BFGS on the same 5 random datasets (including seed 2, the nasty case just discussed). ADAM is worse and does get stuck on the problem case (seed=2), which contradicts the above -- possibly due to early stopping. 
- Experiment 12 tries to rectify the problem via better settings of Lightning's early stopping mechanism. With better early stopping params, ADAM converges 5/5 times and L-BFGS only 4. Maybe we are ready to make it harder. We will use ADAM until this question is revisited, since it works 5/5 times in this experiment.
- Experiment 13 increases the dimension. 2d is easy, 10d is ok, and weirdly, 5d is hard.
- Experiment 14 further increases the dimension to 20, 50, 100 (but makes it easier by decreasing S to 1). Total error is hard to interpret because in higher dimensions, the same per-entry error will result in a larger total error. So, I switched to mean instead of sum over absolute error: `np.abs(Ghat - G).mean()`. (This was previously labeled L2 error but that was a mistake; the code previously read `np.abs(Ghat - G).sum()`, which is L1, not L2.) To see what happens during training, I also added the experiment name and a timestamp to the logger (currently using the pytorch default, which is tensorboard). The results are actually pretty good even up at dimension 100. I partly convinced myself of this by manually checking that the mean absolute value of the error $G-\hat G$ is only about 10% of the mean absolute value of either the true $G$ or the estimate $\hat G$.
- Experiment 15 is like experiment 14, but I set the batch size to be larger than the data (it was supposed to be single-batch all along) and I cranked the dimensions up further (200, 500, 1000). It seems to have worked ok for 200D but bad for 500 and 1000. For 500, the logs seem to show very early termination, but interacting now, I cannot reproduce this. This is still S=1, so we're just doing linear regression with N/P = 10 and no noise. Why would it not work? 
- Experiment 16 took a detour and tested out ADAM versus ADAMW, amsgrad, and L-BFGS. It was a simple little experiment with S=1 and dimension=2. All five got good convergence, but L-BFGS appeared to DOMINATE on runtime. I only logged number of epochs, which could be unfair to ADAM, but in an undocumented followup, I also logged walltime and L-BFGS was 500-fold better. It also never got stuck as seen in experiment 10, maybe because right now S=1 and back then S was 2. Switching S to 2 does seem to reproduce the L-BFGS stuckness on seed=2. 
- Experiment 17 tests L-BFGS vs ADAM with S=1 on a variety of problem dimensions. I realized that the high-D experiments might be getting harder and harder because for each experiment, I scale G to have a max eigenvalue of 1. So the entries get really small by the time we get to dimension 500 or 1000. In terms of walltime, ADAM starts out WAY worse (more than 100-fold), but L-BFGS's advantage decreases with the dimension, and they are nearly on par for 500D. ADAM also seems to scale sub-linearly with the problem dimension, and I would expect that to break down and to not be relevant to the real problem scale. 
- Exp 18: The default memory for L-BFGS in PyTorch is 100. This will naturally slow it down more and more as either the dimension or the number of steps grows beyond 100. It is possible that a good Hessian approximation is less important than other aspects of L-BFGS, such as the Wolfe line search. Thus, experiment 18 tests L-BFGS with some smaller memories. We use dimension 100 and we use S=1 for this experiment. The memory makes little difference to the quality or runtime; perhaps longer memory helps a little with both but the trend is very noisy.
- Exp 19: For high dimensions, during experiment 17, I noticed the problem caused by normalizing the largest eigenvalue to 1. In exp 19 and beyond, I remove that normalization step and G becomes just IID uniform entries with whatever spectrum that implies. This regime yielded very accurate estimates in terms of relative error, even up to 1000D.
- Exp 20: can we now extend this work to yield accurate estimates for S>=2? Experiment 20 tests lots of values of S on a small 10D example. They work very well for S=1 and S=4, but fail badly on S=2,8,16. I accidentally overwrote these results but see exp 21.
- Exp 21: To address pitfall number 1 ($G^2 = (-G)^2$), I repeat experiment 20 but generate $G$ to be closer to the identity matrix ($G = I + U$ where $U$ is iid uniform on $[0,1]$). The L1 penalty shrinks estimates towards $I$. The model is unlikely to recover $-G$ since $-G$ is farther from the identity. At S=1,2,4, this worked nicely. At S=8,16, it mostly did not converge, and at S=16, it mostly stopped immediately. 
- Exp 22: This is exp 21, but with ADAM in place of L-BFGS. It works up to S=8. At S=16, it stops at epoch 31, likely because the early stopping patience param is 30. 
- Exp 23: This is exp 22 but with S=4 fixed and with various dimensions. It worked pretty well up to dimension=50. At dimension=100, it seems promising but it hit the max num epochs (10k) after ~1.5 hours. At dimension 200 and 500, ADAM stopped at bad estimates after thousands of epochs. I was able to interact with the results from the final run (500D). The entries of G^S are large (typical absolute value 8E6), and the relative error in estimating G^S is under 1%. Relative error in estimating $G$ is close to 50%. I suspect Pitfall #1 again, since a matrix with 500 distinct eigenvalues has many 4th roots (500^4 of them). I also notice that the true G has 203 eigenvalues with negative real part, and the estimated G has none. I consider this a success, and I will begin logging errors in $G^S$ alongside errors in $G$, for cases where $G$ is not identifiable.

#### Optimization notes: low-rank linear autoregressive

In early January 2023, I committed changes and began working on additional features. 

- Experiment 24: This was an initial trial covering all three training methods for low-D structure. It included data generated as before, with uniform random controls and uniform random G, and with $2Q=R=[1,1] \otimes I$. It also included data with diagonal $G$ and $X_0=I$ as described above, to make the SVD option exactly correct. 
    - When Q&R are fixed to the correct values, everything works except when $X_0$ is random and $G$ is diagonal. This is not obvious and deserves follow-up.
    - when Q&R are learned via SVD, it works only with $X_0$ identity and $G$ diagonal. This is expected. When we look at $F=RGQ$ instead of $G$, SVD works about as well as fixed-correct-Q, meaning that SVD learns the correct subspace and the model learns the correct dynamics in the high-D space, even if the latent representation is subject to e.g. unknown permutations. 
    - When Q&R are initialized to iid Gaussian and its pseudoinverse and learned via backprop, it fails to learn any of $Q,R,G,F$. Also, the learned R and Q are not inverses. 
- Experiment 25: I changed the initialization to use SVD. The results are much better. All method learn $F$ pretty well when $X_0$ is set to the identity matrix. Generally harder cases: 
    - random $X_0$ is harder than identity
    - diagonal $G$ is harder than random 
    - supervised is harder than SVD or fixed. For supervised QR, each entry of the product QR remains within 0.1 of the identity matrix. This could be better but is not disastrous. 
- Experiment 26: I thought through what I was doing with SVD and rewrote the "Correctness Testing" section further up in this document. I gave up on getting $Q,R$ exactly right and simplified the method of generating simulated data with low-rank $F$, getting rid of the option for diagonal G. Something seems wrong because performance is bad with low-rank F even when the true R is provided. 
- Experiment 27: I slightly altered the low rank structure in $F$, using the same matrix to project both down and up (instead of independent random uniform matrices). This worked much better with the correct R provided, but still failed with SVD or learned structure. I suspect some weird linear algebraic version of vanishing gradients. 
- (lost experiment): when interventions are included, all methods work great. The only problem cases are "SVD" or "supervised" with random initial state, low-rank F, and no interventions. I can see why SVD is not quite correct with random initial state, but "supervised" should be able to fit the data perfectly. 
- Experiment 29: I re-ran the problem case and inspected manually. I noticed that the latent dimension was set incorrectly and I fixed that, but still the recovery of F was bad. I also noticed that without interventional data, the default was to have very little data -- just about constraint per free parameter in the weights of F, but nothing left over to estimate the bias. Adding another group of controls yielded the expected result -- very low error for supervised and good but not exact for SVD. I've run into this before so I will change the default to use more data and rerun and save results for the exp 27 problem cases. 
- Experiment 30: it's time to scale this up and see how much abuse it can take. Using a random $X_0$ and low-rank $F$, I set $S$ to 1,2,4,8,16 and tried all three methods to learn $F$. At S=4 and above, the results are awful. At S=1 and S=2, results are ok. Oracular knowledge of $R$ is best, followed by supervised learning of R and then by learning of R via SVD. Maybe it would be more practical to modify the model so that $X_1 = X_1 + F(X_0)$ instead of $X_1 = F(X_0)$. For full-rank $F$ it doesn't matter because for any $F'$, we can represent $F' + I$ with a suitable choice of $F$. But when $F$ is low rank, we cannot necessarily represent that anymore. 
- Experiment 31: I "ResNettified" it. I changed the lightning module so the initialization for G, and the shrinkage target, are now 0, and also the forward function is now $F(x) + x$ instead of just $F(x)$. When low-rank structure is not used, this should be exactly the same class of models as before. Here I run a basic test, similar to experiment 21, with no low rank structure, to make sure it works the same as before. It is not horrible, but not good and not as good as experiment 21. 
- Experiment 32: the failure in experiment 31 was because I didn't resnetify the simulation code. Once I did that, it looked just as good as experiment 21.
- Experiment 33: Does the low rank stuff work better now that it's all resnettified? No, it's worse. It works with fixed, correct R, but SVD doesn't learn the right R (as expected) and supervised learning of R barely gets past the SVD initialization.
- Experiment 34: i switched the initialization from SVD to oracle. It was almost the same -- not identical, but no real improvement. This may be because even with the exact right R for initialization, G was initialized to the identity matrix and not the true value. I modified the simulation so that G=I and tried again. All methods worked well, though SVD was worse. This is unsatisfying because instead of fixing the inference, I made the problem easier by changing the simulation and cheating on param initialization.
- Experiment 35: Like exp 34, but I stopped cheating on param initialization. It came out the same, so the initializaton cheating didn't help; changing the simulation did help.
- Experiment 36: I made the simulation hard again as in exp. 34, and the results replicated exactly. Upon inspection, the eigenvalues are estimated very badly: many are negative, whereas the true F has no negative eigenvalues. This does not make any sense because I would only expect sign-flips when $S=2$, and here $S=1$. My hypothesis: solving $Y=RGQX$ is similarly difficult to solving $Y=G^3X$, because of the depth. Fixing $G=I$ and just learning $RQ$ might be easier than weakly enforcing $QR=I$ and letting $G$ vary freely. I didn't save these results because I was iterating too fast. `¯\_(ツ)_/¯`
- Experiment 37: Like exp 36, but I went into the pytorch code and fixed G=I when Q,R are learned in a supervised way. This works nicely but it comes at a cost of fucking up the initialization when there is no low-rank structure. So, I will have to re-test that.
- Experiment 38: Like exp 37, but testing non-low-rank F also. As expected, all worked great except SVD.
- Experiment 39: increasing S with F being low-rank plus identity (ResNet structure). This works well up to S=4 and it nukes itself at S=8,16, so that's progress compared to experiment 30, and it's on par with Exp 21. By analogy with experiments 21 and 22, ADAM may help us extend to higher $S$. 
- Experiment 40: like experiment 39 but with ADAM. Indeed, $S=8$ does work with ADAM, where it did not work with L-BFGS. And S=16 blows up. 
- Experiment 41: I set S=1 and scale up the dimension instead, like exp 19. This was a success, though it took a long-ass time (10 hours for 1000D). 
- Experiment 42: Stress test with S=8 and 1000D. Note: latent dimension is 1/2 of data dimension by default, which is much higher that we would use in practice. 